# EnronQA：在私人文档上实现个性化 RAG 的基准解读

> 🇨🇳 中文版 / [🇬🇧 English version](./README_EN.md)
> 论文：**EnronQA: Towards Personalized RAG over Private Documents**（arXiv:2505.00263）  
> 原文链接：https://arxiv.org/abs/2505.00263

本仓库包含：
- 对论文的中文详细解读
- （计划中）基于 EnronQA 的 RAG / LoRA 实验代码和示例

## 使用方式（Quickstart）

```bash
git clone https://github.com/LuoDaa/enronqa-rag-and-memory.git
cd enronqa-rag-and-memory

pip install -r requirements.txt

# 可选：安装 openai，并配置 API key
pip install openai
export OPENAI_API_KEY=你的key

# 跑 BM25 + LLM demo
python -m src.enronqa.bm25_rag


## 目录

- [0. TL;DR](#0-tldr)
- [1. 背景：为什么需要新的 RAG 基准？](#1-背景为什么需要新的-rag-基准)
- [2. EnronQA 是什么？](#2-enronqa-是什么)
- [3. 数据集是怎么构建的？](#3-数据集是怎么构建的)
- [4. 实验一：RAG 校准](#4-实验一rag-校准-enronqa-vs-nq-vs-triviaqa)
- [5. 实验二：RAG Pipeline 基线](#5-实验二rag-pipeline-基线)
- [6. 实验三：记忆-vs-检索](#6-实验三记忆-vs-检索--long-contextrag-与-lora)
- [7. 对工程实践的启发](#7-对工程实践的启发主观总结)
- [8. 如何在自己的项目中使用 EnronQA？](#8-如何在自己的项目中使用-enronqa)
- [9. 总结](#9-总结一句话)




## 0. TL;DR

* 现有 RAG 基准大多基于 **Wikipedia / Web**，现代 LLM 对这些内容已经大量记忆，导致“**不检索也能答对**”，无法真实评估检索质量。
* 这篇论文基于 **Enron 邮件语料** 构建了 **EnronQA**：

  * 103,638 封邮件
  * 528,304 条问答对（QA pairs）
  * 150 个用户收件箱（天然支持个性化 / 多用户场景）
* 作者设计了一个 **多阶段 LLM 流水线** 自动生成高质量问题，并通过四个维度过滤：

  * **Specific（特异）**：问题能唯一指向某封邮件
  * **Objective（客观）**：不同模型在上下文中回答一致
  * **Grounded（扎根）**：不看邮件时模型答不出来
  * **High-Quality（高质量）**：语言清晰、无歧义
* 在 EnronQA 上，**检索召回率的提升几乎线性转化为准确率**，且“无检索”性能很低，说明这个基准真的在考察“检索+生成”，而不是模型记忆。
* 论文还做了一个有趣的 case study：比较 **长上下文（Long Context） vs RAG vs LoRA 记忆**：

  * RAG 整体最好；
  * LoRA 可以在记忆上万条事实时，做到和长上下文差不多的表现，有一定潜力。

---

## 1. 背景：为什么需要新的 RAG 基准？

**Retrieval-Augmented Generation（RAG）** 是目前给大模型加知识的主流方式：
从外部文档检索相关内容，拼进 prompt 里，再由 LLM 生成答案。好处是：

* 不必把私有数据写进模型参数（更隐私安全）；
* 知识更新、权限隔离会更简单。

但现在大家常用的 RAG 基准（NaturalQuestions、TriviaQA 等）大多来自 Wikipedia 或公开网页：

* 现代 LLM 在预训练阶段就看过大量这类内容；
* 结果是：**不检索、直接问模型** 也能得很高分。

这会导致两个问题：

1. **检索质量和最终得分“脱钩”**

   * 你明明换了更好的 retriever，分数却几乎不变；
   * 很难知道自己是调好了检索，还是只是换了个“背书更厉害”的模型。

2. **和真实企业场景差得很远**

   * 真正上线的 RAG 场景多是：邮件、文档、工单、内部 wiki 等 **私有数据**；
   * 而公开基准几乎都在考“百科知识 + 世界常识”，很少关注多用户、权限隔离场景。

**EnronQA** 就是为了解决这两个痛点：
构建一个基于真实邮件、带多用户结构、并且“模型没见过”的 QA 基准。

---

## 2. EnronQA 是什么？

作者基于公开的 **Enron emails corpus**（2015 清洗版）构建了 EnronQA：

* 语料：Enron 公司内部邮件，法律程序公开；
* 原始数据：约 517k 封邮件，150 个用户收件箱；
* 通过过滤后：保留 **103,638 封干净邮件**；
* 基于这些邮件，用 LLM 生成并筛选出 **528,304 条 QA 对**；
* QA 对按 **150 个用户收件箱**切分，天然支持个性化与多租户检索。

从任务角度看，EnronQA 可以用于：

* 文档级 RAG：给定问题，从邮件集合中检索相关邮件并作答；
* 个性化 / 多用户检索：只搜某个用户邮箱、或跨用户搜索；
* 记忆 vs 检索：用 QA 对训练 LoRA 或继续预训练，研究“把知识写进参数” vs “存外部索引”的 trade-off。

---

## 3. 数据集是怎么构建的？

### 3.1 第一步：邮件过滤（Corpus Filtering）

为了把原始邮件变成可发布、适合检索的语料，作者做了一个类似预训练数据清洗的多步流水线，大致包括：

1. **去重（Deduplication）**

   * 使用 minhash + Jaccard 相似度去掉几乎完全重复的邮件；
   * 再做一次“子集去重”：如果一封邮件完全出现在另一封里（完整转发的线程），就删掉那封重复的。

2. **质量过滤（Quality Filters）**

   * 参照 Gopher 的过滤规则，对每封邮件计算：

     * 字数范围（太短/太长都过滤）
     * 平均词长
     * 行尾是省略号的占比
     * 字母与符号的比例
   * 去掉自动日志、模板报表、极短通知等低信息文本。

3. **语言过滤（Language ID）**

   * 使用 fastText 或类似模型做语言识别；
   * 保留高置信度英文邮件，非英语或置信度低的剔除。

4. **NSFW / 毒性过滤**

   * 使用在 Jigsaw 数据上训练的分类器过滤 NSFW、攻击性、敏感内容；
   * 一方面保证基准“可公开”，另一方面也算是对原始参与者隐私的进一步保护。

5. **与 ConcurrentQA 对齐**

   * 为了兼容已有的多跳基准 ConcurrentQA，如果某些被它用到的邮件在上面被过滤掉，会被重新加回来。

通过这一系列操作，最终保留了 **103,638 封高质量邮件** 作为语料库。

---

### 3.2 第二步：基于 LLM 的 QA 生成流水线

接下来要做的是：**把邮件变成问答对**。
作者设计了一个多阶段的 LLM 流水线，大致包含 4 个核心步骤：

1. **初始问题生成（Initial Question Generation）**

   * 输入：一封邮件 + 该邮件下已有的问题列表；
   * 模型：Llama 3.1-70B Instruct；
   * 目标：生成一个新的、单句、可回答、非重复的问题。
   * 使用 DSPy + MIPROv2 自动优化 prompt 和 few-shot 示例。

2. **四重自动评估（Specific / Objective / Grounded / High-Quality）**

   * **Specificity（特异性）**

     * 用向量检索从整个语料中找到 10 封相似邮件；
     * 其中包含“真邮件 + 9 封干扰邮件”；
     * 让 LLM 从 10 封里选出“哪封能回答这个问题”；
     * 如果选不对，说明问题不够具体，可能适用于多个邮件。

   * **Objectivity（客观性）**

     * 用两个不同家族的模型（Llama 70B & Mixtral 8x7B）在给定邮件上下文下回答此问题；
     * 再由一个 judge 模型判断两答案是否一致；
     * 不一致说明问题歧义较大或太主观。

   * **Groundedness（扎根性）**

     * 不给上下文，仅凭模型自身“记忆”来答；
     * 如果这样也能答对，那说明问题太容易猜，不够依赖邮件内容；
     * 通过这个测试可以排除那些“模型背过答案”的问题。

   * **Quality（整体质量）**

     * 先用人工标注一部分问题（高/中/低质量），总结成规则；
     * 再用 Llama 70B + 这些规则构成一个“质量审查”模型。

3. **反馈生成与改写（Feedback & Refinement）**

   * 每一个评估阶段如果失败，都会触发相应的反馈：

     * 不够具体 → 提示“问题需要只针对这一封邮件”；
     * 不够客观 → 提示“需要改写为更清晰可判定的问法”；
     * 不够扎根 → 提示“问题不能在不看邮件时就被猜出”；
     * 质量问题 → 指出违反了哪一条规则。
   * 然后再用这些反馈，让 LLM 重写问题，最多迭代 5 次；
   * 仍然不过，就直接丢弃。

4. **问题释义（Paraphrasing）**

   * 在最终通过所有测试的问题上，再用 Llama 生成一个“释义版问题”；
   * 模型回答新问题，并检查新旧答案是否一致；
   * 一致才保留这个释义版问题，否则重试若干次。

最终，每条 QA 不只有一个问题，而是有 **原问题 + 释义问题** 两个版本。

---

## 4. 实验一：RAG 校准（EnronQA vs NQ vs TriviaQA）

作者首先做了一个很关键的实验：
比较在 3 个数据集上，**检索召回率（Recall@1）提升对准确率的影响**：

* NaturalQuestions（NQ）
* TriviaQA
* EnronQA

实验设置：

* 模型：Llama 3.1-70B；
* 先训练/优化两个 DSPy 程序：

  1. **无上下文**：直接根据问题作答；
  2. **有金标准文档上下文**：把“正确文档”给模型当 context。
* 然后通过采样，模拟不同的 Recall@1：

  * 以某个概率给正确文档；
  * 以剩余概率给随机错误文档；
  * 计算整体准确率曲线。

**结果：**

* 在 NQ / TriviaQA 上：

  * “无上下文”已经能拿到很高分，说明模型记住了大量 Wiki / Web 内容；
  * 要很高的 Recall@1（尤其是 TriviaQA 上接近 0.85），性能才真正超过无上下文基线。
* 在 EnronQA 上：

  * 无上下文准确率非常低（模型没见过这些邮件）；
  * **任何正向的检索改进都会带来准确率稳步、近似线性的提升**；
  * 大致上：Recall 每提升 1%，Accuracy 提升约 0.6%。

> 结论：**EnronQA 是一个“检索敏感”的基准**，适合作为评估 retriever 质量的测试床，而不是只在比“谁背 Wiki 背得多”。

---

## 5. 实验二：RAG Pipeline 基线

在 EnronQA 上，作者给了一个完整的 RAG baseline 对比：

* **检索器**：

  * BM25（PySerini）
  * ColBERTv2
* **LLM**：

  * Llama-3.1-8B Instruct
  * Llama-3.1-70B Instruct
  * GPT-4o
* **RAG 架构**：

  * 直接检索（No Query Rewrite）
  * 先让 LLM 改写问题为搜索 query，再检索（Query Rewrite）

几个重要现象：

1. **BM25 意外地很强**

   * 在 EnronQA 上，BM25 的 Recall@5 能到约 87.5%；
   * 搭配 GPT-4o，无 Query Rewrite 的准确率约 81%。
   * 原因在于：前面 QA 流水线已经强制问题具有“特异性”，问题往往包含邮件里的实体名（人名、公司、项目），这对基于词面的 BM25 特别友好。

2. **Query Rewrite 提升有限**

   * 和很多开放域 QA 工作不同，query rewrite 对 BM25 带来的提升不大，有时甚至略降；
   * 因为原始问题本身就很结构化、信息量充足。

3. **模型越大越好**

   * 在相同检索配置下，8B < 70B < GPT-4o，性能随模型规模单调上升。

对工程侧的启示之一：
**在企业/邮件语料这种高实体密度场景，BM25 依然是一个极强的检索 baseline，可以与向量检索长期并存。**

---

## 6. 实验三：记忆 vs 检索 —— Long Context、RAG 与 LoRA

最后，作者做了一个很有趣的 case study：

> 如果我要让模型“记住一堆事实”，到底是长上下文、RAG 更好，还是用 LoRA 把这些事实写进参数更好？

### 6.1 实验设定

从 EnronQA 中选取一定数量的 QA 对（facts），facts 数量从 **10 到 20,000** 不等。

对每条 QA：

* 原问题 Q
* 释义问题 Q'
* 答案 A

把 Q'–A 看作一条事实，然后分别用三种方式让模型“掌握”这些事实：

1. **Long Context（上下文记忆）**

   * 把所有 Q'–A 以 “Fact 1: Q' → A，Fact 2: …” 这样的形式串成超长上下文；
   * 在末尾加入测试问题 Q，要求模型从 context 中“翻到”对应事实并回答；
   * 因受上下文长度限制，**最多只能塞到 1000 条 facts**。

2. **RAG**

   * 用 ColBERTv2 在 facts 上建索引；
   * 每次给定问题 Q，先检索 top-k（例如 100）条 Q'–A，然后把它们作为 context 喂给模型；
   * 让模型在这些候选 facts 中找到正确答案。

3. **LoRA Memorization（参数记忆）**

   * 选用 Llama-3.1-8B-Instruct 作为基础模型；
   * 用 Q'–A 做 LoRA 微调，让模型把这些事实“背下来”；
   * 超参配置：

     * LoRA rank ∈ {8, 16, 32, 64, 128, 256, 512, 1024, 2048}
     * epoch = 10
     * learning rate = 1e-4
     * LoRA α = 4 × rank
     * dropout = 0.05
     * **给所有 linear 层挂 LoRA**
   * 测试时：只输入问题 Q（不提供 facts），看模型能不能凭参数记忆给出正确答案。

评估时由一个强模型（Llama-3.1-70B）作为 judge，对生成答案打分。

### 6.2 结果与结论

结果（Table 5 + 相关讨论）大致可以总结为：

1. **RAG 在所有规模上都最强**

   * 不论是 10 条 facts 还是 20,000 条 facts，RAG 的准确率都最高；
   * 对于这类“从大量事实中定位一个”的任务，先检索再阅读依然是最稳的方案。

2. **LoRA 可以做到接近 Long Context**

   * 在 facts ≤ 1000 的范围里，多数 LoRA 配置与 Long Context 表现接近；
   * 当事实数量增加到 20,000 时，很多 LoRA 适配器的性能才开始明显下滑；
   * 这说明：**一小块 LoRA 参数里，可以“塞进”相当可观数量的事实**。

3. **Long Context 被上下文长度卡死**

   * 上下文本质上是 O(上下文长度) 的资源，不可能无限扩展；
   * 当事实规模继续增长时，Long Context 方案直接失效，而 RAG / LoRA 还能继续扩展。

作者的整体观点是：

* 在当前技术水平下，**RAG 仍是主力方案**，特别是在私有知识库场景；
* **LoRA 记忆是一个值得继续挖掘的方向**：

  * 适合“高频访问但变化不大”的私有知识；
  * 可以作为 RAG 的补充——把“热知识”记进模型，长尾知识交给检索。

---

## 7. 对工程实践的启发（主观总结）

结合论文内容，我觉得对实际工程有几条比较直接的启发：

1. **评估 RAG 时，要避免“被模型记住”的数据**

   * 用 Wiki / 公开 Web 做 RAG 基准很容易被“模型记忆”污染；
   * 更好的做法是：

     * 使用模型没见过的私有或新鲜数据；
     * 设计类似 Groundedness 的测试：不给上下文时模型答不出来。

2. **LLM + 自动单元测试，可以用来造高质量数据**

   * EnronQA 的 QA 全是合成的，但通过“Specific / Objective / Grounded / High-Quality”四类单元测试，依然能保证质量；
   * 一个可复用的模式是：

     1. 把你想要的数据规格写成可执行的“单元测试”；
     2. 调 prompt / few-shot / 微调，让 LLM 尽量通过这些测试；
     3. 对生成结果做自动过滤，只保留通过所有测试的样本。

3. **LoRA 记忆适合作为 RAG 的补充，而不是替代**

   * 对“稳定且高频”的知识，可以考虑：

     * 用 LoRA 记一部分（减少检索调用、降低延迟）；
     * 继续保留一个 RAG 系统来处理长尾和动态数据。

4. **传统检索（BM25）在企业语料中依然非常有竞争力**

   * 对于实体密集的邮件/内部文档，BM25 往往能给出非常强的 baseline；
   * 稠密检索更适合语义模糊、语言表达多样的场景，但在这类“结构化问答 + 实体匹配”的数据上未必有明显优势。

---

## 8. 如何在自己的项目中使用 EnronQA？

论文里数据集已经开放在 Hugging Face（`MichaelR207/enron_qa_0922`）。如果你想在自己的 GitHub 项目里用，可以这么玩：

* 作为 **私有文档 RAG 的基准**：

  * 练手 BM25 / 向量检索 / 重排；
  * 对比不同 RAG 架构（query rewrite、multi-hop、agentic RAG 等）。
* 做 **个性化检索 / 多租户检索** 实验：

  * 只对单个用户构建索引，模拟“个人邮件助手”；
  * 或按用户聚合成“部门索引”，“组织索引”。
* 做 **记忆相关研究**：

  * 用部分 QA 对做 LoRA 微调、继续预训练；
  * 研究不同记忆机制（Adapter、KV cache、外部 memory 等）的 trade-off。

---

## 9. 总结一句话

> **EnronQA 提供了一个基于真实邮件、面向私有文档和个性化 RAG 的大规模基准，证明了在未被模型预训练污染的数据上，RAG 的性能确实高度依赖检索质量；同时，通过 Long Context / RAG / LoRA 的对比，论文给出了“检索 vs 记忆”的一组有价值的起始实验。**

---

---

## 贡献 / Feedback

欢迎提 issue 讨论这篇论文的更多细节，也欢迎 PR：

- 补充或纠正本文中的解读
- 增加基于 EnronQA 的检索 / LoRA / 长上下文实验代码
- 添加英文版解读

如果你觉得这个仓库对你有帮助，也欢迎点一下 ⭐。


